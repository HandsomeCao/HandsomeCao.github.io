<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>docker使用</title>
      <link href="/2018/05/13/docker/"/>
      <url>/2018/05/13/docker/</url>
      <content type="html"><![CDATA[<h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p><a href="www.docker.com">Docker</a>是一个<strong>开源</strong>的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。按理说，Docker并不是专门用于深度学习的工具，它运用非常广泛，对于任何编程的项目，Docker都能提供很好的帮助。</p><p>在实验室中需要使用Docker的原因主要是为了</p><blockquote><p>防止在服务器中相互影响和破坏底层环境，从而使用Docker为每个人生成一个虚拟的单独的环境</p></blockquote><p>这是相当有用的，对于每个生产环境，都可以生产一个单独的<strong>容器</strong>, 同时与其他生产环境相隔离。这和虚拟机似乎有点类似，但相比较于虚拟机，Docker所生成的容器具有<strong>更快速，更轻量</strong> 的效果。在Docker中有两个很重要的概念</p><ul><li><strong>容器</strong>(container)，其对应于面向对象方法中的<strong>对象</strong></li><li><strong>镜像</strong>(image)， 其对应于面向对象方法中的<strong>类</strong></li></ul><p>所以使用Docker的过程通常为：</p><ol><li>自己或者找到一个别人配好的适合自己生产环境的镜像。通常自己配镜像是通过<strong>DockerFile</strong>文件</li></ol><pre><code class="lang-bash">列出本机的所有 image 文件。$ docker image ls删除 image 文件$ docker image rm [imageName]</code></pre><p>image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。</p><pre><code class="lang-bash"># pull 命令从官网抓取hello-world镜像$ docker image pull library/hello-world</code></pre><ol><li>利用得到的image文件，生成容器实例</li></ol><pre><code class="lang-bash">$ docker container run hello-world</code></pre><p>在docker中最常用的就是<code>docker run</code>命令，也是最重要的。这里就用实验室所用的<code>docker run</code>命令展现其每个参数的意义</p><pre><code class="lang-bash">$ docker run -p 7981:8888 -it --name=[容器名称] -v /home/cao/workspace:/root/workspace --device /dev/nvidia-uvm --device /dev/nvidia0 --device /dev/nvidia1 [镜像名称] /bin/bash</code></pre><ul><li><strong>-p</strong> : 端口映射，即容器的8888端口映射到本机的7981端口</li><li><strong>-it</strong>  : 表示进入容器之后，进入命令行交互模式</li><li><strong>—name</strong> : 指定生成容器的名称，<em>一定要指定</em>。</li><li><strong>-v</strong>  : 路径映射，即容器的<em>/root/workspace</em>， 与本机的<em>/home/cao/workspace</em> 相互挂载，所以不能轻易删除</li><li><strong>—device</strong> : 映射本机的指定显卡</li></ul><p>这样在生成一个容器之后，通常就只需要对此容器进行操作，而生产环境全由<code>docker attach [容器名称]</code>进入容器之中操作。使用<code>docker -h</code>可查看全部帮助。</p>]]></content>
      
      
        <tags>
            
            <tag> docker 工具 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络</title>
      <link href="/2018/05/13/cnn-basic/"/>
      <url>/2018/05/13/cnn-basic/</url>
      <content type="html"><![CDATA[<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><blockquote><p> <strong>卷积层</strong>是卷积神经网络(<em>Convolutional Neural Network</em>)中的基本操作。其使用一个<strong>卷积核</strong>通过对输入层进行卷积操作，从而可以提取到图像中的高层信息。</p></blockquote><p>假设输入图像是下图中的一个$6*6$的矩阵,其中每个格代表了图像点的像素值。</p><p><img src="https://i.imgur.com/In87wix.png" alt=""></p><p>设置其卷积核（<em>kernel</em>）为$3*3$的矩阵，卷积核中的参数不同，可导致其卷积得到的结果不同:</p><p><img src="https://i.imgur.com/Dn0HGFv.png" alt=""></p><p>同时，假定卷积操作每做一次卷积，卷积核移动一个像素位置，即卷积的<strong>步长</strong>(<em>stride</em>)为1。第一次卷积操作从图像$(0,0)$像素开始，由卷积核中的参数与对应位置图像像素<strong>逐位相乘后累加</strong>作为一次卷积结果。</p><p><img src="https://i.imgur.com/OyxUEt2.png" alt=""></p><p>对于<strong>卷积核1</strong>(<em>filter1</em>)，其对于$(0,0)​$位置进行卷积操作:</p><script type="math/tex; mode=display">Conv_{out}=\sum{a_{i}b_{i}=}1*1+0*(-1)+0*(-1)+0*(-1)+1*1+0*(-1)+0*(-1)+1*1=3</script><p>当步长为$1$时，卷积核按照补偿大小在输入图像<strong>从左到右从上到下</strong>依次将卷积操作进行下去，最终输出一个$4<em>4$大小的<em>*卷积特征</em></em>，同时这卷积特征将作为下一层操作的输入。</p><p><img src="https://i.imgur.com/NdLlBtZ.png" alt=""></p><p>与之类似，若三维情形下的卷积层$l$的输入张量为$x^{l}\in R^{H^{l}\times W^{l} \times D^{l}}$,该层的卷积核为$f^{l}\in R^{H^{l}\times W^{l} \times D^{l}}$。三维输入时，卷积操作实际上只是将二维卷积扩展到了相应位置的所有通道上，最终将一次卷积处理的所有$HWD^{l}$个元素求和作为该位置的卷积结果。</p><p>若进一步，类似$f^{l}$这样的卷积核有$D$个，则在同一个位置可得到$1 \times 1 \times 1 \times D$维度的卷积输出，而$D$即为第$l+1$层特征$x^{l+1}$的通道数$D^{l+1}$。对于三维图像，形式化的卷积操作为:</p><script type="math/tex; mode=display">y_{i^{l+1},j^{l+1},d}=\sum_{i=0}^{H}\sum_{j=0}^{W}\sum_{d^{l}=0}^{D^{l}}f_{i,j,d^{l},d} \times x_{i^{l+1}+i, j^{l+1}+j,d^{l}}^{l}</script><p>其中$(i^{l+1},j^{l+1})$为卷积结果的位置坐标，在卷积层中，$f$可视作学习到的网络中的权重(<em>weight</em>)，可以发现该项权重对不同位置的所有输入都是相同的，也就是一个卷积核时作用于不同的区域的，这也就是卷积神经网络中的<strong>权值共享</strong>特性，当然除此之外，也可以为卷积操作设定其神经元中的<strong>偏置项</strong>，当然也可将其设置为0。</p><p><strong>零填充</strong>：</p><hr><p>可见，对于上面介绍的卷积操作，对于一张输入图像，进行不断的卷积操作，得到的输出特征尺寸将在不断<strong>减小</strong>，有时这样是不可取的，因为最终的输出逐渐减小后，所学到的图像特征也所剩无几了。所以这里有个保证图像输出不再减小的方法，称为<strong>零填充<em>(Zero padding)</em></strong>。</p><p><img src="https://i.imgur.com/7ccpHke.png" alt=""></p><p>如上图所示，即是在卷积操作中设置$padding=1$，也就是在边缘像素周围再以1个0填充。对于本来的输入为$6 \times6$大小的图像，以卷积核为$3 \times 3$，步长为$1$进行卷积，会得到$4 \times 4$大小的输出图像。但当<strong>加上零填充</strong>之后，输入图像也就可看作是$8 \times 8$大小，进行同样的卷积操作，输出却得到了$6 \times 6$大小的图像，和输入保持不变，这样也就能<strong>增加网络中的卷积操作，学到更为高层的特征</strong>。</p><p>所以在卷积操作中有三个重要的<strong>超参数</strong>(<em>Hyper parameters</em>)：</p><ul><li><strong>卷积核大小(<em>filter size</em>)</strong>: $(f \times f)$</li><li><strong>卷积步长(<em>filter stride</em>)</strong>：$s$</li><li><strong>零填充(<em>padding</em>)</strong>: $p$</li></ul><p>对于输入图像大小为$n \times n$，对于<strong>输出尺寸</strong>可以得到如下的一般公式:</p><script type="math/tex; mode=display">\lfloor\frac{n+2p-f}{s}+1\rfloor \times \lfloor\frac{n+2p-f}{s}+1\rfloor</script><p>当然，合适的超参数设置会对模型带来意想不到的效果提升。在<strong>pytorch</strong>中，对于二维图像的卷积操作，使用<code>nn.Conv2d</code>表示。</p><h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><blockquote><p>在卷积神经网络中，池化层往往在卷积层的后面，通过池化来<strong>降低</strong>卷积层输出的特征向量，同时改善结果，防止过拟合。</p></blockquote><p>池化通常用的两种方法为：<strong>最大值池化</strong>(Max-Pooling)和<strong>平均值池化</strong>(Average-Pooling)。</p><ul><li><strong>最大值池化</strong>：</li></ul><script type="math/tex; mode=display">y_{i^{l+1},j^{l+1},d}=max(x_{i^{l+1} \times H+I, j^{l+1} \times W+j,d^{l}}^{l})</script><ul><li><strong>平均值池化</strong>:</li></ul><script type="math/tex; mode=display">y_{i^{l+1},j^{l+1},d}=\frac{1}{HW}\sum x_{i^{l+1} \times H+I, j^{l+1} \times W+j,d^{l}}^{l}</script><p>对于卷积后得到的图像特征，<em>Max-Pooling</em>可形象表示为:</p><p><img src="https://i.imgur.com/yyCG6On.png" alt=""></p><p>这里池化的核大小选择为$(2 \times 2)$，也就是对于第一个块中，选择最大值7得到输出值。当然，如果是平均值池化，也就是取四个数的平均值作为其池化后的值。</p><p>可以发现，池化操作后的结果相比其输入减小了，其实际上是一种<strong>降采样(<em>down-sampling</em>)</strong>，池化层的引入是按照人的视觉系统对视觉输入对象进行降采样和抽象，其主要作用主要有以下三项：</p><ol><li><strong>特征不变性</strong>：池化操作使模型更关注是否存在某些特征而不是特征具体的位置。可看作是一种很强的先验，使特征学习包含某种程度自由度，能容忍一些特征微小的位移。</li><li><strong>特征降维</strong>：由于池化操作的降采样作用，汇合结果中的一个元素对应于原输入数据的一个子区域，因此池化相当于在空间范围内做了维度约减，从而使模型可以抽取更广范围的特征。同时减小了下一层输入大小，进而减小计算量和参数个数。</li><li>在一定程度上<strong>防止过拟合</strong>，方便优化。</li></ol><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p><img src="https://i.imgur.com/vh9DPQ7.png" alt=""></p><blockquote><p> LeNet-5可以说是最早的卷积神经网络结构了，它主要用于手写数字识别的任务上，其发表于1998年的<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">论文</a></p></blockquote><p><strong>结构</strong>：LeNet-5不包括输入层一共有7个层，每一层都包含了可以训练的参数，输入为一张$32 \times 32$的图像。</p><ol><li><strong>C1卷积层</strong></li></ol><p>这一层的输入就是原始的图像，输入层接受图片的输入大小为$32 \times32 \times1$。卷积层的核（过滤器）尺寸为$5\times5$，深度为$6$，不使用$0$进行填充，步长为$1$。通过计算公式可以求出输出的尺寸为$28\times28\times6$，卷积层的深度决定了输出尺寸的深度。卷积层总共的参数有$5<em>5</em>1<em>6+6 =156$个参数，加的$6$为卷积后的偏置项参数。本层所拥有的节点有$28</em>28<em>6=4704$个节点， 而本层的每一个节点都是经过一个$5\times5$的卷积和一个偏置项计算所得到的，$5</em>5+1=26$，所以本层卷积层一共有$4704*26 = 122304$个连接。</p><ol><li><strong>S2池化层</strong></li></ol><p>本层的输入是C1的输出，它接收一个$28\times28\times6$大小的矩阵。在卷积神经网络中，常有的池化方法为最大池化和平均池化，由于使用的核为$2\times2$，步长也为$2$，意味着每四个相邻元素经过S2之后会得到一个输出，所以输出矩阵大小变为$14\times14\times6$大小。</p><ol><li><strong>C3卷积层</strong></li></ol><p>本层卷积操作，采用的卷积核仍为$5\times5$，使用$16$个卷积核，也就是深度为$16$，同样不使用零填充，步长为$1$，所以得到输出大小为$10\times10\times16$。</p><ol><li><strong>S4池化层</strong></li></ol><p>本层采用同S2相同的池化操作，使输出值再缩小一半量级，变为$5\times5\times16$。</p><ol><li><strong>C5卷积层</strong></li></ol><p>C5层由$120$个卷积核组成，一个卷积与<strong>S4</strong>中每一个<em>feature map</em>（$5<em>5</em>16$）相连，所以每一个C5的卷积核都会输出一个$1<em>1$的矩阵，所以在S4与C5之间是可看作属于全连接。C5是一个卷积层而不是一个全连接层，如果这个LeNet5的输入变的更大了而其它的保持不变，那么这个输出将要大于$1</em>1$。</p><ol><li><strong>F6全连接层</strong></li></ol><p>F6层包含了$84$个结点，计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过<strong>sigmoid</strong>函数，来产生一个输出，也就是之前的输入为$120$个神经元，现在通过一层全连接，其中含$84$个神经元，来将输出降维。 </p><ol><li><strong>输出层</strong></li></ol><p>输出层是由<strong>欧式径向基函数（RBF）</strong>组成。每一个输出对应一个RBF函数，每一个RBF函数都有$84$维的输入向量.。每一个RBF函数都会有一个输出，最后输出层会输出一个10维的向量。，以映射到10个数字分别的预测的准确度。</p><p><strong>MNIST数据集</strong></p><p>MNIST数据集是入门的第一个数据集，其数据都为$28\times28$大小的<strong>单通道</strong>图像，它含有$60000$张训练图片，和$10000$张验证图片。其形式非常简单，图像如下图</p><p><img src="http://wiki.jikexueyuan.com/project/tensorflow-zh/images/mnist_digits.png" alt=""></p><p>用<strong>LeNet-5</strong>对MNIST数据集进行训练，可以取得<strong>92%</strong>左右的效果，已比传统机器学习方法实现的效果更好。</p><p><img src="http://yann.lecun.com/exdb/lenet/gifs/a35.gif" alt=""></p><p>通过pytorch实现的LeNet结构</p><pre><code class="lang-python">class LeNet(nn.Module):    def __init__(self):        super(LeNet, self).__init__()        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)        self.conv2 = nn.Conv2d(6, 16, 5)        # self.conv3 = nn.Conv2d(16, 120, 5)        self.fc1 = nn.Linear(16*5*5, 120)        self.fc2 = nn.Linear(120, 84)        self.fc3 = nn.Linear(84, 10)    def forward(self, x):        out = F.relu(self.conv1(x))  # 28*28*6        out = F.max_pool2d(out, 2)  # 14*14*6        out = F.relu(self.conv2(out))  # 10 *10 *16        out = F.max_pool2d(out, 2)  # 5*5*16        out = out.view(out.size(0), -1)        out = F.relu(self.fc1(out))        out = F.relu(self.fc2(out))        out = self.fc3(out)        return out</code></pre><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><blockquote><p>在LeNet-5之后，卷积神经网络由于硬件，数据等元素的局限，训练较为困难，所以发展一直都是不温不火，直到<strong>AlexNet</strong>在2012年<strong>ImageNet</strong>竞赛中以超越第二名10.9个百分点的优异成绩一举夺冠，从而打响了卷积神经网络、乃至深度学习在计算机视觉领域中研究热潮的“第一枪”。</p></blockquote><p><strong>结构</strong>：</p><p><img src="https://i.imgur.com/et352X8.png" alt=""></p><p>在AlexNet的网络结构中，共含五层卷积层和三层全连接层。AlexNet的上下两支是为了方便同时使用两片GPU进行训练，不过在第三层卷积和全连接层处上下两支信息可交互。由于两支网络完全一致，只需对其中一支进行分析。</p><ol><li><strong>卷积层C1</strong>：输入图像为$3\times224\times224$大小，实际是$3\times227\times227$大小，卷积核为$11<em>11$，不使用零填充，步长为$4$，总共使用$96$个卷积核，由公式可得$\lfloor\frac{227-11}{4}+1\rfloor=55$，即由卷积得到$96\times55\times55$大小的特征图。采用$3</em>3$尺度，步长为$2$去池化，则池化后的图像尺寸为$27$，所以像素规模为$96\times27\times27$，由于采用两个GPU，则分为两组，每组大小为$48\times27\times27$。</li><li><strong>卷积层C2</strong>: 对于上一层的输出，采用$256$个$3*3$大小的卷积核，使用$1$个padding，步长为$1$，对其进行卷积得到$256\times26\times26$的特征图，对其同样采用最大池化，上下两层分别得到$128\times13\times13$大小的输出特征图。</li><li><strong>卷积层C3</strong>：此层采用$384$个$3*3$大小的卷积核，同样使用$1$个零填充，上下两层分别得到$192\times13\times13$大小的输出图。</li><li><strong>卷积层C4</strong>: 同上一层，此层采用$384$个$3*3$大小的卷积核，同样使用$1$个零填充，上下两层分别得到$192 \times13\times13$大小的输出图。</li><li><strong>卷积层C5</strong>：此层采用$256$个$3*3$大小的卷积核，同样使用$1$个零填充，上下两层分别得到$128\times12\times12 $大小的输出图。再采用最大池化，降维到$128\times 6 \times 6$大小。</li><li><strong>全连接层</strong> ： 之后共采用了三层全连接结构，对于其中一个GPU, 特征大小逐渐从$128<em>6</em>6$到$2048$，再从$2048$到$2048$，最后将两个GPU合并，也就是将特征从$4096$映射到$1000$，也就是ImageNet中类别的数量。</li></ol><p><strong>贡献</strong>:</p><ul><li><strong>AlexNet</strong>  首次将卷积神经网络应用于计算机视觉领域的海量图像数据集<strong>ImageNet</strong>， 揭示了卷积神经网络拥有强大的学习能力和表达能力。另一方面海量的数据也能防止神经网络过拟合。自此引发深度学习井喷式增长。</li><li>利用<strong>GPU</strong>实现网络训练，之前由于计算资源的发展受限，阻碍了神经网络的研究进程。如今，利用GPU已大大减少了大型网络模型开发的成本和时间。</li><li>一些训练技巧为之后研究打下了基础。<strong>ReLU激活函数</strong>，<strong>局部响应规范化</strong>(LRN)操作(如今已不常用)，随机失活(<strong>Dropout</strong>: 随机再网络中去除一些连接)，这些训练技巧不仅保证了模型的性能，也为之后深度卷积神经网络构建提供了范本。</li></ul><p><strong>代码实现</strong>:</p><pre><code class="lang-python">class AlexNet(nn.Module):    def __init__(self, num_classes=1000):        super(AlexNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),            nn.ReLU(inplace=True),            LRN(local_size=5, alpha=0.0001, beta=0.75),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Conv2d(96, 256, kernel_size=5, padding=2, groups=2),            nn.ReLU(inplace=True),            LRN(local_size=5, alpha=0.0001, beta=0.75),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Conv2d(256, 384, kernel_size=3, padding=1),            nn.ReLU(inplace=True),            nn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2),            nn.ReLU(inplace=True),            nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(kernel_size=3, stride=2),        )        self.classifier = nn.Sequential(            nn.Linear(256 * 6 * 6, 4096),            nn.ReLU(inplace=True),            nn.Dropout(),            nn.Linear(4096, 4096),            nn.ReLU(inplace=True),            nn.Dropout(),            nn.Linear(4096, num_classes),        )    def forward(self, x):        x = self.features(x)        x = x.view(x.size(0), 256 * 6 * 6)        x = self.classifier(x)        return x</code></pre><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>VGG网络有类似Alex的形式，但它在一些地方也有不同。</p><ul><li>VGG-Net中普遍使用了小卷积核，AlexNet多采用的大于5的卷积核，而VGG的卷积核多为$3*3$</li><li>网络卷积层的通道数从$3\to64\to128\to256\to512$。通道数逐渐变得很大，学到的特征多。</li><li>VGG中的卷积很多都保持了输入大小，也就是卷积后大小保持不变，为的是在增加网络深度时确保各层输入大小随深度增加而不极具减小。</li></ul><p><strong>结构</strong></p><p><img src="https://i.imgur.com/t4U5sHK.png" alt=""></p><p><img src="https://i.imgur.com/rQwN1Zn.png" alt=""></p><h3 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a>ResNets</h3><blockquote><p> 理论和实验表明，神经网络的<strong>深度</strong>和<strong>宽度</strong>是表征网络复杂度的两个核心因素，不过深度相比宽度在增加网络复杂性上更加有效，然而随着深度的增加，训练会变得愈加困难。这主要是因为在基于<strong>随机梯度下降</strong>的网络训练过程中，误差的多层反向传播会导致<strong>梯度弥散</strong>或<strong>梯度爆炸</strong>。可能随着网络的训练，误差并未减少而却增加。<strong>残差网络</strong>(ResNets)便很好地解决了这一问题。</p></blockquote><p><strong>残差块</strong></p><p><img src="https://i.imgur.com/qxiUrhA.png" alt=""></p><p>残差网络主要受<strong>高速网络</strong>的影响，假设某卷积神经网络有$L$层，其中第$i$层的输入为$x^{i}$，参数为$w^{i}$，该层的输出为$y^{i}=x^{i+1}$，忽略偏置，则之间关系表示为:</p><script type="math/tex; mode=display">y=F(x,w)</script><p>其中，$F$为非线性激活函数，而对于高速网络来言，$y$的计算定义如下:</p><script type="math/tex; mode=display">y=F(x,w)*(T(x,w)+x*C(x,w)</script><p>其中$T,C$是两个非线性变换,分别称作“<strong>变换门</strong>”和“<strong>携带门</strong>”。变换门负责控制变换的强度，携带门则控制原输入信号的保留强度，由于增加了<strong>保留原输入数据的可能性</strong>，所以这种网络会更加灵活。而残差网络可以看作其的特殊情况：本来优化目标为:</p><script type="math/tex; mode=display">y=F(x,w)+x</script><p>简单变形为</p><script type="math/tex; mode=display">F(x,w)=y-x</script><p>也就是说，网络所要学习的就是<strong>残差项</strong>$y-x$。残差块有两个学习分支，其一是左侧的残差函数，其二为右侧对输入的恒等映射。这两个分支经过简单的整合，再经过一个非线性变换<strong>ReLU</strong>，从而形成网络的残差块。由多个残差块堆积而成了<strong>残差网络</strong>。</p><p><img src="https://img-blog.csdn.net/20161028170505110" alt=""></p><ul><li>利用pytorch构建残差块</li></ul><pre><code class="lang-python"># Residual blockclass ResidualBlock(nn.Module):    def __init__(self, in_channels, out_channels, stride=1, downsample=None):        super(ResidualBlock, self).__init__()        self.conv1 = conv3x3(in_channels, out_channels, stride)        self.bn1 = nn.BatchNorm2d(out_channels)        self.relu = nn.ReLU(inplace=True)        self.conv2 = conv3x3(out_channels, out_channels)        self.bn2 = nn.BatchNorm2d(out_channels)        self.downsample = downsample    def forward(self, x):        residual = x        out = self.conv1(x)        out = self.bn1(out)        out = self.relu(out)        out = self.conv2(out)        out = self.bn2(out)        if self.downsample:            residual = self.downsample(x)        out += residual # 加上残差项        out = self.relu(out)        return out</code></pre><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><blockquote><p>由于增加网络的深度和宽度会导致训练难以进行下去，<strong>Inception</strong>主要思路是用<strong>密集成分来近似最优的局部稀疏结构</strong>去加深网络的同时，增<strong>宽</strong>网络结构。</p></blockquote><p><strong>Inception模块</strong></p><p><img src="https://img-blog.csdn.net/20160225155351172" alt=""></p><ul><li>采用不同大小的卷积核，意味着得到不同大小的感受野，最后<strong>拼接</strong>起来意味着不同尺度的融合。</li><li>之所以卷积核采用1，3，5，主要为了方便对齐。设定步长$stide=1$之后，只要设定相应的$padding$为0， 1， 2，那么卷积之后即可得到相同维度的特征，那么这些特征即可拼接再一起。</li><li>在一个方向上先采用了最大化池化，这样能得到更好的效果。</li><li>采用了$1*1$卷积</li><li>网络越到后面，特征越抽象，而且每个特征涉及的感受野也更大了，因此随着层数的增加，$3<em>3$和$5</em>5$卷积的比例也要增加。</li></ul><p>由多个<strong>Inception</strong>模块构建成了<strong>GoogLeNet</strong></p><p><img src="https://img-blog.csdn.net/20160225155403967" alt=""></p><p>在Inception-v1之后为了提高训练速度和效果出现了许多衍生版本，但思想都不变，理解了Inception模块，就嫩理解Inception网络。</p><p>Inception模块的pytorch实现</p><pre><code class="lang-python">class Inception_base(nn.Module):    def __init__(self, depth_dim, input_size, config):        super(Inception_base, self).__init__()        self.depth_dim = depth_dim        #mixed &#39;name&#39;_1x1        self.conv1 = nn.Conv2d(input_size, out_channels=config[0][0], kernel_size=1, stride=1, padding=0)        #mixed &#39;name&#39;_3x3_bottleneck        self.conv3_1 = nn.Conv2d(input_size, out_channels=config[1][0], kernel_size=1, stride=1, padding=0)        #mixed &#39;name&#39;_3x3        self.conv3_3 = nn.Conv2d(config[1][0], config[1][1], kernel_size=3, stride=1, padding=1)        # mixed &#39;name&#39;_5x5_bottleneck        self.conv5_1 = nn.Conv2d(input_size, out_channels=config[2][0], kernel_size=1, stride=1, padding=0)        # mixed &#39;name&#39;_5x5        self.conv5_5 = nn.Conv2d(config[2][0], config[2][1], kernel_size=5, stride=1, padding=2)        self.max_pool_1 = nn.MaxPool2d(kernel_size=config[3][0], stride=1, padding=1)        #mixed &#39;name&#39;_pool_reduce        self.conv_max_1 = nn.Conv2d(input_size, out_channels=config[3][1], kernel_size=1, stride=1, padding=0)        self.apply(helpers.modules.layer_init)    def forward(self, input):        output1 = F.relu(self.conv1(input))        output2 = F.relu(self.conv3_1(input))        output2 = F.relu(self.conv3_3(output2))        output3 = F.relu(self.conv5_1(input))        output3 = F.relu(self.conv5_5(output3))        output4 = F.relu(self.conv_max_1(self.max_pool_1(input)))        return torch.cat([output1, output2, output3, output4], dim=self.depth_dim)</code></pre>]]></content>
      
      
        <tags>
            
            <tag> CNN 卷积神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络基础</title>
      <link href="/2018/05/12/nn-basic/"/>
      <url>/2018/05/12/nn-basic/</url>
      <content type="html"><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>如图，一个<strong>神经元通常有如下的结构</strong>：<img src="https://i.imgur.com/BT6bd4A.png" alt="Snipaste_2018-05-08_14-41-31"></p><p>对于一组输入${a_{1}, a_{2}….,a_{k}}$，分别施于不同的<strong>权重</strong>(weight)，再通过加上一个<strong>偏置</strong>(bias)，得到神经元的<strong>线性模型</strong>,也就是之前学过的线性规划:</p><script type="math/tex; mode=display">z=a_{1}w_{1}+a_{2}w_{2}+......+a_{k}w_{k}+b</script><p>用向量化表示为:</p><script type="math/tex; mode=display">z=w^{T}a+b</script><p>由于线性变化所能解决的问题并不多，所以在输出施加一个<strong>激活函数</strong>(<em>activate function</em>)， 以在方程模型中引入非线性，同时另外的作用是可对输出进行限制:</p><script type="math/tex; mode=display">a_{out}=\delta(z)=\delta(w^{T}a_{in}+b)</script><ul><li><strong>实例</strong></li></ul><p><img src="https://i.imgur.com/ZKWIH2G.png" alt="Snipaste_2018-05-08_14-59-43"></p><p>对于上方的神经元实例，输入$a=[2, -1, 1]^{T}$,对于其三个输入分别施加的权重为$w=[1,-2, -1]^{T}$，将偏置<em>bias</em>设置为$1$，激活函数选择<strong>Sigmoid</strong>函数$\delta(z)=\frac{1}{1+e^{-z}}$，所以计算结果可以得到:</p><script type="math/tex; mode=display">a_{out}=\delta(w^{T}x+b)=\frac{1}{1+e^{-(2*1+(-1)*(-2)+1*(-1)+1)}}=0.98</script><h3 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h3><p>对于许多神经元，将其组合起来，对于网络每一层，设置不同数量的神经元，即可得到一个<strong>全连接前馈神经网络</strong>：</p><p><img src="https://i.imgur.com/7ivKJwZ.png" alt="Snipaste_2018-05-08_15-11-01"></p><p>全连接神经网络通过输入的特征向量$x$，得到输出$y$，而网络层中的权重$w$和偏置$b$即是，网络中的参数。在训练过程中，可通过<strong>反向传播</strong>和<strong>梯度下降算法</strong>进行更新。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>这里介绍三种常见激活函数：</p><ul><li><p><strong>Sigmoid</strong>:</p><script type="math/tex; mode=display">f(z)=\frac{1}{1+e^{-z}}</script><p>其图像可表示为:</p><p><img src="https://i.imgur.com/IRPHg7V.png" alt=""></p></li></ul><p>很明显，可以看出经过<strong>Sigmoid</strong>函数作用后，输出响应的值域被压缩到了[0,1]之间，这也是<strong>逻辑回归</strong>中用到它的原因。对<strong>Sigmoid</strong>函数求导：</p><script type="math/tex; mode=display">\frac{d}{dz}f(z)=\frac{e^{-z}}{(1+e^{-z})^{2}}</script><p>对梯度画出图可见:</p><p><img src="https://i.imgur.com/v9Vx5WH.png" alt="sigmoid_梯度"></p><p><strong>不足之处</strong>：</p><ol><li>当$z$大于5或者小于-5时，部分的梯度<strong>接近于0</strong>，这会导致在误差反向传播中，导数处于该区域内的误差很难传播到前层，进而影响整个网络导致其无法训练。</li><li>从<strong>Sigmoid</strong>函数中可以看出其值域的均值都大于0，而并非<strong>等于</strong>0，这也不满足神经网络内对数值的期望。</li></ol><ul><li><strong>Tanh：</strong></li></ul><script type="math/tex; mode=display">f(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}</script><p><img src="https://i.imgur.com/vBRVjtq.png" alt="tanh"></p><blockquote><p>Tanh函数在Sigmoid函数的基础上解决了<strong>均值问题</strong>。</p></blockquote><p><strong>Tanh</strong>函数又称作<strong>双曲正切函数</strong>，其函数范围为$(-1,1)$，输出的响应均值为0。<strong>Tanh</strong>函数与<strong>Sigmoid</strong>函数的关系为:</p><script type="math/tex; mode=display">Tanh(z)=2Sigmoid(2z)-1</script><p>所以，求<strong>Tanh</strong>的导数:</p><script type="math/tex; mode=display">\frac{d}{dz}Tanh(z)=4Sigmoid(2z)*Sigmoid(2z)^{'}=\frac{4e^{-2z}}{(1+e^{-2z})^{3}}</script><p>具体地：</p><script type="math/tex; mode=display">\frac{d}{dz}Tanh(z)=1-(tanh(z))^{2}</script><p>由于<strong>Tanh</strong>函数仍基于<strong>Sigmoid</strong>函数，所以使用它仍依然会有<strong>“梯度饱和”</strong>现象。</p><ul><li><strong>ReLU</strong></li></ul><blockquote><p>为了避免<strong>梯度饱和</strong>现象的发生，在神经网络中引入了修正线性单元(<em>Rectified Linear Unit</em>)。</p></blockquote><script type="math/tex; mode=display">ReLU(x)=max(0,x)</script><script type="math/tex; mode=display">\begin{equation}ReLU(x)=\begin{cases}x& x\ge0\\0& x<0\end{cases}\end{equation}</script><p><img src="https://i.imgur.com/1lU9PNR.png" alt="relu"></p><p><strong>ReLU</strong>的导数:</p><script type="math/tex; mode=display">\begin{equation}\frac{d}{dx}ReLU(x)=\begin{cases}1& x >0\\0& x<0\\undefined&x=0\end{cases}\end{equation}</script><p>与前两个激活函数相比，<strong>ReLU</strong>的梯度在$x\ge0$时为$1$，反之为$0$，对$x\ge0$部分完全消除了之前的梯度饱和效应。计算复杂度上，<strong>ReLU</strong>函数也相比之前的两种指数函数简单，实验中还发现其有助于随机梯度下降方法收敛。<strong>ReLU</strong>函数已是目前深层卷积神经网络中最为常用的激活函数。</p>]]></content>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>回归分析</title>
      <link href="/2018/05/11/regression/"/>
      <url>/2018/05/11/regression/</url>
      <content type="html"><![CDATA[<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><blockquote><p> <strong>回归分析</strong>是一种统计学上分析数据的方法，目的在于了解两个或多个变量是否相关、相关方向与程度，并建立数学模型以便观察特定变量来预测研究者感兴趣的变量。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。</p></blockquote><p><strong>线性回归</strong>可以说是机器学习中最简单的模型，但是其实际地位很重要。先通过<strong>房价预测</strong>的例子来了解线性回归。</p><hr><p>假设有一个房屋销售的数据如下</p><div class="table-container"><table><thead><tr><th>房屋面积(m^2)</th><th>销售价钱(万元)</th></tr></thead><tbody><tr><td>123</td><td>250</td></tr><tr><td>150</td><td>320</td></tr><tr><td>87</td><td>160</td></tr><tr><td>102</td><td>220</td></tr><tr><td>……….</td><td>………..</td></tr></tbody></table></div><p>我们用几何图表示出这样的数据</p><p><img src="https://i.imgur.com/0lc9czo.png" alt=""></p><p>当我们有很多组这样的数据时，这些就是训练数据，我们希望学习一个模型，当有新的一个面积数据来到时，可以自动预测出销售价钱。也就是我们可以在这张图上用一条直线去尽量拟合这些数据，当有新的值来到时，可以用这条直线所对应的值去返回预测的价钱， 绿色的点就是我们所想要预测的点。当然，不是说线性回归就一定是一条直线，当变量x是一维的时候才是一条直线，而在高维时，是<strong>超平面</strong> 。</p><p><img src="https://i.imgur.com/BRBoV1l.png" alt="pic"></p><p>在这里先定义下<strong>数学符号</strong>，我们用$X=(x_{1}, x_{2}, x_{3},…,x_{n})^{T}$来表示输入数据矩阵，其中$x_{i}\in R^{p}$表示一个p维度长的数据样本，$y = (y_{1}, y_{2}, ….,y_{n}) \in R^{n}$表示数据的标签。</p><p>线性回归的模型可以表示为， 对于一个样本$x_{i}$，它的输出值是其特征的线性组合：</p><script type="math/tex; mode=display">f(x_{i}) = \sum_{m=1}^{p}w_{m}x_{im} + w_{0} = w^{T}x_{i}</script><p>其中<strong>$w_{0}$</strong>称为截距，或者<em>bias</em> 。线性回归的目标是用预测结果尽可能地拟合目标<em>label</em> 。对于机器学习模型，需要定义一个<strong>损失函数</strong>(Loss function)，用它来表示其与真实输出之间的误差， 从而评判模型的好坏。在这里定义如下的损失函数：</p><script type="math/tex; mode=display">J(w) = \frac{1}{2}\sum_{i=1}^{n}(h_{w}(x^{i})-y^{i})^{2}</script><p>这个错误估计函数是去对$x^{i}$的估计值与真实值$y^{i}$差的平方和作为错误估计函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了, 从而方便之后的求导计算。为了训练模型，去<strong>最小化误差</strong> ， 使其接近于0， 这里需要用到的方法称为<strong>梯度下降法</strong> 。先用张图表示:</p><p><img src="https://i.imgur.com/tYp7YPW.png" alt=""></p><blockquote><p>梯度下降法，就是要一步步沿着梯度反向方向逐渐下降，从而走到局部甚至全局最小值。</p></blockquote><p>梯度下降法是按以下流程逐渐进行的：</p><ol><li>首先对$w$赋值， 这个值可以是随机的，也可以让其是个全零的向量。</li><li>改变$w$的值，使得$J(w)$按梯度下降的方向进行减少。这一步，就需要先求得每个参数的梯度，再通过梯度，逐渐改变其值。<strong>数学公式</strong>表示为:</li></ol><script type="math/tex; mode=display">\frac{\partial}{\partial w}J(w) = \frac{\partial}{\partial w}\frac{1}{2}\sum_{i=1}^{n}(h_{w}(x)-y)^{2}=(h_{w}(x)-y)x^{(i)}</script><ol><li>求得梯度后，就能通过设置一个学习率$\alpha$来沿着梯度减少的方向变化:</li></ol><script type="math/tex; mode=display">w_{i} := w_{i}-\alpha\frac{\partial}{\partial w_{i}}J(w)</script><p>代入上式的损失函数和所计算的梯度，也就得到:</p><script type="math/tex; mode=display">w_{i} = w_{i} - \alpha\frac{\partial}{\partial w_{}}J(w) = w_{i} - \alpha(h_{0}(x)-y)x^{(i)}</script><p>在真实计算时，往往参数使用矩阵或向量表示:</p><script type="math/tex; mode=display">\nabla J = \begin{vmatrix} \frac{\partial}{\partial w_{1}} J \\ \frac{\partial}{\partial w_{2}} J \\ ... \\ \frac{\partial}{\partial w_{n}} J\end{vmatrix}</script><script type="math/tex; mode=display">w = w - \alpha \nabla_{w} J</script><ul><li>线性回归实现: </li></ul><pre><code class="lang-python"># Linear Regression Modelclass LinearRegression(nn.Module):    def __init__(self, input_size, output_size):        super(LinearRegression, self).__init__()        self.linear = nn.Linear(input_size, output_size)      def forward(self, x):        out = self.linear(x)        return out</code></pre><hr><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><blockquote><p>逻辑回归也称为对数几率回归。明明叫回归，但却是<strong>分类问题</strong>中极为重要的手段。其思想也是基于线性回归，属于<strong>广义线性回归</strong>模型。</p></blockquote><p>对于二分类问题来讲，给你一个输入特征向量$X$，它可能对应一张图片，你想识别看它是否是一张猫的图片， 通过一个算法输出预测$\hat{y}$， 也就是对于实际值$y$的估计。换句话说，也就是想通过输入图片$X$， 想通过输出$\hat{y}$来知道这张图片是猫的几率有多大。用$w$来表示逻辑回归的参数，$x$是一个$n_{x}$维的特征向量，$b$表示这个模型中所要用到的偏置。如果用<strong>线性回归</strong>的方法来表示这个模型:</p><script type="math/tex; mode=display">\hat{y} = wx + b</script><p>这时候我们得到一个关于输入$x$的线性函数，但是对于这个二元分类来讲，似乎并不是一个很好的算法。因为我们想要得到的是这张图片是否为猫的概率，所以输出值应该介于<em>0 ~ 1</em>之间，而通过<strong>线性回归</strong>输出的$\hat{y}$可能要比1大很多，甚至为负值，这样来说这个模型就没有意义了，因此在<strong>逻辑回归</strong>当中，我们应将输出局限在<em>0~1</em>之间， 所以增加了一个<em>Sigmoid</em>函数将线性函数转换为非线性函数。</p><script type="math/tex; mode=display">\delta(z) = \frac{1}{1+e^{-z}}</script><p>使用<strong>matplotlib</strong>画出<em>Sigmoid</em>的图形表示如下，可以看出输出值都在<em>0~1</em>之间</p><pre><code class="lang-python"># 画sigmoidimport matplotlib.pyplot as pltimport numpy as npimport math%matplotlib inlinedef sigmoid(x):    a = []    for item in x:        a.append(1/(1+math.exp(-item)))    return ax = np.arange(-10, 10, 0.2)sig = sigmoid(x)plt.plot(x, sig)plt.show()</code></pre><p><img src="https://i.imgur.com/IRPHg7V.png" alt=""></p><p>如果$z$非常大那么$e^{-z}$将会接近于0，那么<strong>sigmoid</strong>函数的值将会近似等于1除以1加上某个非常接近于0的项，因为$e$ 的指数如果是个绝对值很大的负数的话，这项将会接近于0，所以如果$z$很大的话那么关于$z$的<strong>sigmoid</strong>函数会非常接近1。相反地，如果$z$非常小或者说是一个绝对值很大的负数，那么关于$e^{-z}$这项会变成一个很大的数，你可以认为这是1除以1加上一个非常非常大的数，所以这个就接近于0 。</p><p>所以要将识别猫这个任务所得到的结果规定在<em>0~1</em>之间，那么久需要对刚才定义的线性模型增加一个<strong>sigmoid</strong>函数，使其变为非线性:</p><script type="math/tex; mode=display">\hat{y} = \delta(wx+b)</script><script type="math/tex; mode=display">\delta(z) = \frac{1}{1+e^{-z}}</script><p>也就是:</p><script type="math/tex; mode=display">\hat{y}(x)=\frac{1}{1+e^{-(wx+b)}}</script><p>有了这个模型，要去实现这个识别猫的分类任务，接下来要做的就是通过给定的数据集，通过训练模型，把$w$参数给找出来。要找模型中的权重，就需要先定义<strong>损失函数</strong>。那么怎么去找到能衡量这个二分类的损失函数，通过使用<strong>极大似然估计</strong>。由于所要判断出的图片，只有两种可能:</p><ul><li>1表示图片里是猫</li><li>0表示图片里不是猫</li></ul><p>所以这两种情况的概率分别为:</p><script type="math/tex; mode=display">P(y=1|x; w) = \phi(w^{T}x+b)=\phi(z)</script><script type="math/tex; mode=display">P(y=0|x;w)=1-\phi(w^{T}x+b)=1-\phi(z)</script><p>根据上面两式，通过<strong>最大似然估计</strong>求解损失函数，首先得到<strong>概率函数</strong>为:</p><script type="math/tex; mode=display">P(y|x;w)=\phi(z)^{y}(1-\phi(z))^{(1-y)}</script><p>因为数据集中样本数据是相互独立的，所以它们的联合分布可以表示为总的乘积:</p><script type="math/tex; mode=display">L(w)=\prod_{i=1}^{m}P(y^{i}|x^{i};w)</script><script type="math/tex; mode=display">L(w) =\prod_{i=1}^{m}\phi(z^{i})^{y^{i}}(1-\phi(z^{i}))^{1-y^{i}}</script><p>取<strong>对数似然函数</strong>:</p><script type="math/tex; mode=display">l(w)=ln(L(w))=\sum_{i=1}^{m}ln(\phi(z^{i})^{y^{i}})+ln(1-\phi(z^{i}))^{(1-y^{i})}</script><script type="math/tex; mode=display">l(w)=ln(L(w))=\sum_{i=1}^{m}y^{i}ln(\phi(z^{i}))+(1-y^{i})ln(1-\phi(z^{i}))</script><p>最大似然估计就是要取使$l(w)$最大时的$w$,所以在前面加上一个<strong>负号</strong>不就是使求其最小了吗？这样就得到损失函数:</p><script type="math/tex; mode=display">J(w) = -l(w)=-y^{i}ln(\phi(z^{i}))-(1-y^{i})ln(1-\phi(z^{i}))</script><p>所以简化形式的损失函数为:</p><script type="math/tex; mode=display">L(\hat{y}, y)=-yln(\hat{y})-(1-y)ln(1-\hat{y})</script><p>当$y=1$时损失函数$L=-ln(\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能大，因为<strong>sigmoid</strong>函数取值$[0,1]$，所以$\hat{y}$会无限接近于1。</p><p>当$y=0$时损失函数$L=-ln(1-\hat{y})$，如果想要损失函数$L$尽可能得小，那么$\hat{y}$就要尽可能小，因为<strong>sigmoid</strong>函数取值$[0,1]$，所以$\hat{y}$会无限接近于0。</p><p>这只是对于单个样本的损失函数，对于总的样本，需将所有代价加起来除以m：</p><script type="math/tex; mode=display">J(w)=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y}, y)</script><p>接下来要做的就是通过<strong>梯度下降法</strong>求得导数，再如之前一样更新参数$w$和$b$的值即可。</p><p>同样使用<strong>pytorch</strong>来实现逻辑回归模型:</p><pre><code class="lang-python">class LogisticRegression(nn.Module):    def __init__(self, input_size, num_classes):        super(LogisticRegression, self).__init__()        self.linear = nn.Linear(input_size, num_classes)    def forward(self, x):        out = self.linear(x)        out = torch.sigmoid(out)        return out</code></pre>]]></content>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
